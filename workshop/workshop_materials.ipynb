{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Evaluating LLM based applications\n",
    "\n",
    "It is so easy and quick to build a shiny PoC using LLMs and it is so hard to turn it into a production-grade LLM application. To succeed you need a robust evaluation framework, which you are going to use during the development and post-deployment of your LLM based app.\n",
    "\n",
    "This workshop consists of 4 main parts:\n",
    "* evaluation-driven development and architecture of a LLM based app\n",
    "* evaluation framework for a LLM based app\n",
    "* test suite with evals for a LLM based app \n",
    "* monitoring foundations for a LLM based app\n",
    "\n",
    "**About workshop giver**\n",
    "\n",
    " With over 19 years of experience in Data and AI, [Una Galyeva](https://www.linkedin.com/in/unagalyeva/) held various positions, from hands-on Data and AI development to leading Data and AI teams and departments. As a driving force behind [PyLadies Amsterdam](https://amsterdam.pyladies.com/), a Microsoft MVP, Women in AI Benelux Advisory board member, and the owner of AI MLOps Agency, Una is passionate about challenging perspectives and inspiring others to see things differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation-driven development and architecture of a LLM based app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation-driven development\n",
    "\n",
    "**Evaluation-driven development** is a methodology to guide the development of LLM based apps via a set of task-specific evaluations. This term is inspired by test-driven development in software engineering. \n",
    "\n",
    "![Evaluation Driven Development Workflow](../assets/EDD.png)\n",
    "\n",
    "*Source: [Evaluation-driven development workflow](https://docs.databricks.com/en/generative-ai/tutorials/ai-cookbook/evaluation-driven-development.html) by Databricks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "A LLM based app architecture can be quite complex. It starts small and could be expanded step by step by adding new components.\n",
    "* basic LLM app: a user query is passed directly to a LLM model, its response is passed back to the user\n",
    "* enhanced context: LLM model is given access to external data and tools for creating more informed responses\n",
    "* guardrails in place to protect both the LLM app and its users\n",
    "* model router and gateaway to support complex pipelines and enhance security\n",
    "* performance optimization to reduce latency and costs with caching\n",
    "* agent patterns to incorporate complex logic and actions to maximize the LLM app capabilities\n",
    "\n",
    "![Architecure of a LLM based app](../assets/LLM_app_arch.png)\n",
    "\n",
    "*Source: Chapter 10 of [AI Engineering](https://www.oreilly.com/library/view/ai-engineering/9781098166298/) by Chip Huyen*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation framework for a LLM based app\n",
    "\n",
    "TODO base descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and dataset prep\n",
    "\n",
    "TODO add info about our usecase, steps and brief descr of Evidently lib \n",
    "\n",
    "Each evaluation that computes a score for every text in the dataset is called a **descriptor**. Descriptors can be numerical or categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from evidently import ColumnMapping\n",
    "from evidently.descriptors import *\n",
    "from evidently.metric_preset import TextEvals\n",
    "from evidently.metrics import *\n",
    "from evidently.report import Report\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests import *\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop you will work with a question answering dataset that imitates an internal company Q&A system (created to answer employees' questions about HR, finance, etc). \n",
    "\n",
    "Import it using requests library, convert into pandas data frame, parse dates and set 'start_time' as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://raw.githubusercontent.com/pyladiesams/eval-llm-based-apps-jan2025/main/assets/QA.csv\")\n",
    "qa_csv_content = BytesIO(response.content)\n",
    "qa_logs = pd.read_csv(qa_csv_content, index_col=0, parse_dates=['start_time', 'end_time'])\n",
    "qa_logs.index = qa_logs.start_time\n",
    "qa_logs.index.rename('index', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a preview of the first three rows of the dataset to familiarize yourself with it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "qa_logs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working with Evidently it is highly recommended to map the data schema to make sure that it is parsed correctly.\n",
    "\n",
    "To handle this, create a column mapping by identifying the type of columns and pointing to a \"datetime\" column for adding a time index to your plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = ColumnMapping(\n",
    "    datetime='start_time',\n",
    "    datetime_features=['end_time'],\n",
    "    text_features=['question', 'response'],\n",
    "    categorical_features=['organization', 'model_ID', 'region', 'environment', 'feedback'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export the evaluation results beyond viewing the visual Reports in Python. Currently Evidently supports export to a data frame, Python dictionary, JSON and HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO add proper transition paragraph\n",
    "\n",
    "If your LLM solves a classification or retrieval task, you can evaluate classification or ranking quality. See available Presets, Metrics, and Tests to see other checks you can run.\n",
    "\n",
    "TODO check links to the presets, metrics and tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text statistics\n",
    "\n",
    "Computes descriptive text statistics by evaluating simple properties like text length, sentence count, word count, percentage of out-of-vocabulary words, percentage of non-letter characters.\n",
    "\n",
    "**Evaluate text length**\n",
    "\n",
    "Generate a Report to evaluate the length of each text in the *response* column. Run this check for the first 200 rows of the *qa_logs* dataframe.\n",
    "\n",
    "This calculates the number of symbols in each text and shows a summary. You can see the distribution of the text length across all responses and descriptive statistics like the mean or minimal text length. \n",
    "\n",
    "Click on Details to see how the mean text length changes over time. The index comes from the *datetime* column you mapped earlier. This helps you to notice any temporal patterns, such as if texts are longer or shorter during specific periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  TextLength(),\n",
    "                  ]\n",
    "              )\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get a side-by-side comparison**. \n",
    "\n",
    "You can also generate statistics for two datasets at once. For example, compare the outputs of two different prompts or data from today against yesterday.\n",
    "\n",
    "Pass one dataset as a reference one and another as a current one. For simplicity, let's compare the text length for the first and next 100 rows from the same dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  TextLength(),\n",
    "                  ]\n",
    "              )\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=qa_logs[:100],\n",
    "                      current_data=qa_logs[100:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Get a side by side comparison of the sentence count for the first and the next 100 rows from the same dataframe. Use SentenceCount() descriptor.\n",
    "Consult the descriptor docs [here](https://docs.evidentlyai.com/reference/all-metrics#descriptors-text-stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  ,\n",
    "                  ]\n",
    "              )\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=qa_logs[:100],\n",
    "                      current_data=qa_logs[100:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text patterns\n",
    "\n",
    "Detect specific words or regular patterns by using regular expressions behind the scenes. Such evals are faster and cheaper to compute at scale. \n",
    "\n",
    "For example, check if the responses mention competitors, banned or forbidden words, include emails or other links. Text pattern descriptors return a binary score (\"True\" or \"False\") for pattern matches.\n",
    "\n",
    "Let's check if *responses* contains specific words related to the compensation (such as salary, benefits, or payroll). Pass this word list to the IncludesWords() descriptor. This will also check for word variants. Add an optional display name for this eval.\n",
    "\n",
    "You can see that 18 responses out of 200 relate to the topic of compensation as defined by this word list. Details show occurrences in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  IncludesWords(\n",
    "                      words_list=['salary', 'benefits', 'payroll'],\n",
    "                      display_name=\"Mention Compensation\")\n",
    "            ]\n",
    "        ),\n",
    "        ]\n",
    ")\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Get a side by side comparison of the excluded words (competitor, offer) for the first and the next 100 rows from the same dataframe. Add \"No competitor offer\" display name for this eval. Use ExcludesWords() descriptor.\n",
    "Consult the descriptor docs [here](https://docs.evidentlyai.com/reference/all-metrics#descriptors-text-patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "            ]\n",
    "        ),\n",
    "        ]\n",
    ")\n",
    "\n",
    "text_evals_report.run(reference_data=qa_logs[:100],\n",
    "                      current_data=qa_logs[100:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-based evaluation\n",
    "\n",
    "Uses pre-trained machine learning models for evaluation. \n",
    "\n",
    "Evidently has built-in model-based descriptors and wrappers to call external models published on Hugging Face.\n",
    "\n",
    "**Semantic similarity**\n",
    "\n",
    "You can evaluate how closely two texts are in meaning using an embedding model. SemanticSimilarity() descriptor calculates pairwise semantic similarity between columns for each pair of text. You can compare the text from *Response* and *Question* columns to see if the answers are semantically relevant to the question.\n",
    "\n",
    "SemanticSimilarity() descriptor converts all texts into embeddings, measures Cosine Similarity between them, and returns a score from 0 to 1:\n",
    "* 0 means that texts are opposite in meaning;\n",
    "* 0.5 means that texts are unrelated;\n",
    "* 1 means that texts are semantically close.\n",
    "\n",
    "In this case, the semantic similarity always stays above 0.81, which means that answers generally relate to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\", descriptors=[\n",
    "        SemanticSimilarity(with_column=\"question\", \n",
    "                           ),\n",
    "    ])\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment analysis**\n",
    "\n",
    "Analyzes the sentiment of the text using a word-based model. Returns score on a scale: -1 (negative) to 1 (positive). Shows the distribution of the response sentiment. Allows you to spot specific times when the average sentiment of the responses dipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "Execute a sentiment check on the first 200 responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\", descriptors=[\n",
    "            ,\n",
    "        ]\n",
    "    ),\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toxicity**\n",
    "\n",
    "The toxicity measurement aims to quantify the toxicity of the input texts using a pretrained hate speech classification model. In this model, 'hate' is defined as abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation. This model returns a predicted toxicity score between 0 and 1. In each case, the descriptor first downloads the model from Hugging Face to your environment and then uses it to score the data. It takes a few moments to load the model. The higher the score the more toxic is your response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\", descriptors=[\n",
    "            HuggingFaceToxicityModel(),\n",
    "        ]\n",
    "    ),\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge\n",
    "\n",
    "For more complex or nuanced checks, you can use LLMs as a judge. This requires creating an evaluation prompt asking the same or more powerful LLMs to assess the text by specific criteria, such as tone or conciseness.\n",
    "\n",
    "TODO add info on pros and cons of LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata summary\n",
    "\n",
    "The QA dataset has a *feedback* column which includes user upvotes and downvotes. You can easily enrich your Report with summaries from any numerical or categorical columns.\n",
    "\n",
    "Use ColumnSummaryMetric() to add a summary of the *feedback* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_report = Report(metrics=[\n",
    "   ColumnSummaryMetric(column_name=\"feedback\"),\n",
    "   ]\n",
    ")\n",
    "\n",
    "feedback_report.run(reference_data=None, \n",
    "                    current_data=qa_logs[:200], \n",
    "                    column_mapping=column_mapping)\n",
    "feedback_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test suite with evals for a LLM based app\n",
    "\n",
    "TODO base descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you've used Reports to summarize evaluation outcomes. However, you need to set specific conditions for the metric values to run automated tests (such as check if all texts fall within the expected length range) and review results only if something goes wrong.\n",
    "\n",
    "This is where you can use an alternative interface called **TestSuites**. TestSuites work similarly to Reports, but instead of listing metrics, you define tests and set conditions using parameters like gt (greater than), lt (less than), eq (equal), etc. \n",
    "\n",
    "**Define a Test Suite**\n",
    "\n",
    "Add tests to check the following conditions:\n",
    "* response length is always non-zero\n",
    "* maximum response length does not exceed 1800 symbols (e.g., due to chat window constraints).\n",
    "* mean response length is above 500 symbols (e.g., this is a known pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suite = TestSuite(tests=[\n",
    "    TestColumnValueMin(column_name = TextLength().on(\"response\"), gt=0),\n",
    "    TestColumnValueMax(column_name = TextLength().on(\"response\"), lte=1800),\n",
    "    TestColumnValueMean(column_name = TextLength().on(\"response\"), gt=500),\n",
    "])\n",
    "\n",
    "test_suite.run(reference_data=None, \n",
    "                    current_data=qa_logs[:200], \n",
    "                    column_mapping=column_mapping)\n",
    "test_suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Enrich the current test suite with a new condition: average response sentiment is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suite = TestSuite(tests=[\n",
    "    TestColumnValueMin(column_name = TextLength().on(\"response\"), gt=0),\n",
    "    TestColumnValueMax(column_name = TextLength().on(\"response\"), lte=1800),\n",
    "    TestColumnValueMean(column_name = TextLength().on(\"response\"), gt=500),\n",
    "    ,\n",
    "])\n",
    "\n",
    "test_suite.run(reference_data=None, \n",
    "                    current_data=qa_logs[:200], \n",
    "                    column_mapping=column_mapping)\n",
    "test_suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Test Suite**\n",
    "\n",
    "You can start by re-using available tests presets, later you can design a custom Test Suite by picking up specific Tests and setting conditions more precisely. Here how can you do it:\n",
    "\n",
    "1. Choose individual Tests: select the tests you want to include in your Test Suite.\n",
    "2. Pass Test parameters: set custom parameters for applicable Tests\n",
    "3. Set custom conditions: define when Tests should pass or fail.\n",
    "4. Mark Test criticality: mark non-critical Tests to give a Warning instead of Fail. \n",
    "\n",
    "More extended information can be found [here](https://docs.evidentlyai.com/user-guide/tests-and-reports/run-tests#custom-test-suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Suite usage**\n",
    "\n",
    "* *regression testing*: run test suites whenever you modify any part of your LLM system, such as trying a new retrieval strategy, model version, or prompt. The goal is to check that updates don't make the quality of generative outputs worse or introduce new errors. You compare new responses with references or against set of criteria.\n",
    "* *continuous testing*: run test suites periodically over production logs to check that the output quality stays within expectations.\n",
    "\n",
    "You can also set up alerts to get a notification if your Tests contain failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring foundations for a LLM based app\n",
    "\n",
    "TODO base descr, info on tracing (spans), recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "TODO create summary from all parts with main take-aways\n",
    "\n",
    "**Further resources**\n",
    "\n",
    "TODO add list of the resources\n",
    "\n",
    "**Used materials**\n",
    "\n",
    "TODO add list of used materials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
